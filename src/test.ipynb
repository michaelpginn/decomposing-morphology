{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sep>\n",
      "<pad>\n",
      "???\n",
      "A1P\n",
      "A1S\n",
      "A2P\n",
      "A2S\n",
      "ADJ\n",
      "ADV\n",
      "AFE\n",
      "AFI\n",
      "AGT\n",
      "AP\n",
      "APLI\n",
      "ART\n",
      "CAU\n",
      "CLAS\n",
      "COM\n",
      "COND\n",
      "CONJ\n",
      "DEM\n",
      "DIM\n",
      "DIR\n",
      "E1\n",
      "E1P\n",
      "E1S\n",
      "E2\n",
      "E2P\n",
      "E2S\n",
      "E3\n",
      "E3P\n",
      "E3S\n",
      "ENF\n",
      "EXS\n",
      "GNT\n",
      "IMP\n",
      "INC\n",
      "INS\n",
      "INT\n",
      "ITR\n",
      "ITS\n",
      "MED\n",
      "MOV\n",
      "NEG\n",
      "NOM\n",
      "NUM\n",
      "PART\n",
      "PAS\n",
      "PL\n",
      "POS\n",
      "PP\n",
      "PREP\n",
      "PRG\n",
      "PRON\n",
      "REC\n",
      "RFX\n",
      "S\n",
      "SAB\n",
      "SC\n",
      "SREL\n",
      "SV\n",
      "TAM\n",
      "TOP\n",
      "TRN\n",
      "VI\n",
      "VOC\n",
      "VT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[0, 0, 0, 0],\n",
       "  [1, 0, 0, 0],\n",
       "  [2, 0, 0, 0],\n",
       "  [3, 1, 1, 2],\n",
       "  [3, 1, 1, 1],\n",
       "  [3, 2, 1, 2],\n",
       "  [3, 2, 1, 1],\n",
       "  [4, 0, 0, 0],\n",
       "  [5, 0, 0, 0],\n",
       "  [6, 0, 0, 0],\n",
       "  [7, 0, 0, 0],\n",
       "  [8, 0, 0, 0],\n",
       "  [9, 0, 0, 0],\n",
       "  [10, 0, 0, 0],\n",
       "  [11, 0, 0, 0],\n",
       "  [12, 0, 0, 0],\n",
       "  [13, 0, 0, 0],\n",
       "  [14, 0, 0, 0],\n",
       "  [15, 0, 0, 0],\n",
       "  [16, 0, 0, 0],\n",
       "  [17, 0, 0, 0],\n",
       "  [18, 0, 0, 0],\n",
       "  [19, 0, 0, 0],\n",
       "  [20, 1, 2, 0],\n",
       "  [20, 1, 2, 2],\n",
       "  [20, 1, 2, 1],\n",
       "  [20, 2, 2, 0],\n",
       "  [20, 2, 2, 2],\n",
       "  [20, 2, 2, 1],\n",
       "  [20, 3, 2, 0],\n",
       "  [20, 3, 2, 2],\n",
       "  [20, 3, 2, 1],\n",
       "  [21, 0, 0, 0],\n",
       "  [22, 0, 0, 0],\n",
       "  [23, 0, 0, 0],\n",
       "  [24, 0, 0, 0],\n",
       "  [25, 0, 0, 0],\n",
       "  [26, 0, 0, 0],\n",
       "  [27, 0, 0, 0],\n",
       "  [28, 0, 0, 0],\n",
       "  [29, 0, 0, 0],\n",
       "  [30, 0, 0, 0],\n",
       "  [31, 0, 0, 0],\n",
       "  [32, 0, 0, 0],\n",
       "  [33, 0, 0, 0],\n",
       "  [34, 0, 0, 0],\n",
       "  [35, 0, 0, 0],\n",
       "  [36, 0, 0, 0],\n",
       "  [37, 0, 0, 0],\n",
       "  [38, 0, 0, 0],\n",
       "  [39, 0, 0, 0],\n",
       "  [40, 0, 0, 0],\n",
       "  [41, 0, 0, 0],\n",
       "  [42, 0, 0, 0],\n",
       "  [43, 0, 0, 0],\n",
       "  [44, 0, 0, 0],\n",
       "  [45, 0, 0, 0],\n",
       "  [46, 0, 0, 0],\n",
       "  [47, 0, 0, 0],\n",
       "  [48, 0, 0, 0],\n",
       "  [49, 0, 0, 0],\n",
       "  [50, 0, 0, 0],\n",
       "  [51, 0, 0, 0],\n",
       "  [52, 0, 0, 0],\n",
       "  [53, 0, 0, 0],\n",
       "  [54, 0, 0, 0],\n",
       "  [55, 0, 0, 0]],\n",
       " [56, 4, 3, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import MT5Tokenizer\n",
    "from functools import reduce\n",
    "import features \n",
    "\n",
    "dataset = datasets.load_dataset(\"lecslab/usp-igt\")\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\", legacy=False)\n",
    "\n",
    "# Collect the unique set of gloss labels\n",
    "all_glosses = sorted(set([gloss for glosses in dataset['train']['pos_glosses'] +\n",
    "                            dataset['eval']['pos_glosses'] +\n",
    "                            dataset['test']['pos_glosses'] for gloss in glosses.replace(\"-\", \" \").split()]))\n",
    "all_glosses = [\"<sep>\", \"<pad>\"] + all_glosses\n",
    "for gloss in all_glosses:\n",
    "    print(gloss)\n",
    "\n",
    "feature_map = features.create_feature_map('../features_v1.csv', all_glosses)\n",
    "feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'E3S': 3118, 'E1P': 1370, 'E1S': 709, 'E2S': 564, 'E3': 385, 'A1S': 347, 'A2S': 127, 'A1P': 110, 'E3P': 32, 'E2P': 16, 'E2': 16, 'E1': 2, 'A2P': 2})\n",
      "Counter({'E3S': 59, 'E2S': 17, 'A1S': 13, 'E1S': 10, 'A1P': 1, 'E1P': 1, 'E2P': 1, 'A2S': 1})\n",
      "Counter({'E3S': 295, 'E2S': 76, 'E1S': 42, 'A1S': 22, 'E1P': 9, 'A2S': 3, 'E3': 1, 'E2P': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for d in ['train', 'eval', 'test']:\n",
    "    all_glosses = []\n",
    "    for gloss_row in dataset[d]['pos_glosses']:\n",
    "        all_glosses += filter(lambda g: g in ['A1P', 'A1S', 'A2P', 'A2S', 'E1', 'E1P', 'E1S', 'E2', 'E2P', 'E2S', 'E3', 'E3P', 'E3S'], gloss_row.replace(\"-\", \" \").split())\n",
    "    print(Counter(all_glosses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6efe5a4c7d46efba8d9fd8b172f6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['transcription', 'segmentation', 'pos_glosses', 'glosses', 'translation'],\n",
       "    num_rows: 104\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rows = dataset['train'].filter(lambda r: 'A1P' in r['pos_glosses'].replace('-', ' ').split())\n",
    "filtered_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 unique glosses\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'transcription': 'o sey xtok rixoqiil',\n",
       " 'segmentation': \"o' sea x-tok r-ixÃ³qiil\",\n",
       " 'pos_glosses': 'CONJ ADV COM-VT E3S-S',\n",
       " 'glosses': 'o sea COM-buscar E3S-esposa',\n",
       " 'translation': 'O sea busca esposa.',\n",
       " 'input_ids': [259, 268, 303, 276, 259, 329, 11207, 1418, 329, 159121, 696, 1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [19, 0, 8, 0, 17, 66, 0, 31, 56, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "SEP_TOKEN_ID = all_glosses.index(\"<sep>\")\n",
    "PAD_TOKEN_ID = all_glosses.index(\"<pad>\")\n",
    "print(f\"{len(all_glosses)} unique glosses\")\n",
    "\n",
    "def encode_gloss_labels(label_string: str):\n",
    "    \"\"\"Encodes glosses as an id sequence. Each morpheme gloss is assigned a unique id.\"\"\"\n",
    "    word_glosses = label_string.split()\n",
    "    glosses = [word_gloss.split(\"-\") for word_gloss in word_glosses]\n",
    "    glosses = [[all_glosses.index(gloss) for gloss in word if gloss != ''] for word in glosses]\n",
    "    glosses = reduce(lambda a, b: a + [SEP_TOKEN_ID] + b, glosses)\n",
    "    return glosses + [PAD_TOKEN_ID]\n",
    "\n",
    "def tokenize(batch):\n",
    "    inputs = tokenizer(batch['transcription'], truncation=True, padding=False, max_length=64)\n",
    "    inputs['labels'] = [encode_gloss_labels(label) for label in batch['pos_glosses']]\n",
    "    return inputs\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels tensor([  19,    0,    8,    0,   17,   66,    0,   31,   56,    1, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n",
      "Features tensor([[  19,    0,    8,    0,   17,   66,    0,   31,   56,    1, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
      "Epoch time = 0.000s / row\n",
      "Features tensor([[  19,    0,    8,    0,   17,   66,    0,   31,   56,    1, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "# collator(batch.select_columns(['input_ids', 'attention_mask', 'labels']).to_list()\n",
    "\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset['train'].select_columns(['input_ids', 'attention_mask', 'labels']), batch_size=32, collate_fn=collator)\n",
    "\n",
    "for batch in dataloader:\n",
    "    # print(batch['input_ids'].shape)\n",
    "    print(\"Labels\", batch['labels'][0])\n",
    "    print(\"Features\", batch['labels'].unsqueeze(-2)[0])\n",
    "    f = features.map_labels_to_features(batch['labels'],  [[i] for i in range(len(all_glosses))])\n",
    "    print(\"Features\", f[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18,  0,  7,  0, 16, 65,  0, 30, 55]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(dataset['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2],\n",
       "        [5, 5]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def greedy_decode(feature_map, feature_logits: List[torch.Tensor]):\n",
    "        \"\"\"Decodes a bundle of feature logits into gloss ID predictions. \n",
    "        The output should align with the input vocabulary of the decoder.\n",
    "\n",
    "        Args:\n",
    "            feature_logits (List[torch.Tensor]): List of feature logit tensors each of size `(batch_size, seq_length, feature_size)`\n",
    "        \"\"\"\n",
    "        batch_size = feature_logits[0].shape[0]\n",
    "        seq_length = feature_logits[0].shape[1]\n",
    "\n",
    "        primary_features = torch.argmax(feature_logits[0], -1)\n",
    "\n",
    "        def _decode_row(row_index: int):\n",
    "            \"\"\"Decodes the tokens in a given row of the batch. Generator function that `yields` a single prediction (id) for each token.\"\"\"\n",
    "            for token_index in range(seq_length):\n",
    "                # Filter based on the primary feature\n",
    "                possible_label_ids = [index for index, value in enumerate(\n",
    "                    feature_map) if value[0] == primary_features[row_index, token_index].item()]\n",
    "\n",
    "                if len(possible_label_ids) == 1:\n",
    "                    yield possible_label_ids[0]\n",
    "                    continue\n",
    "\n",
    "                # Now we have a list of possible feature matrices\n",
    "                # Compute the joint probability based on the logits of each feature\n",
    "                softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "                # (num_features, feature_size)\n",
    "                feature_probs = [softmax(feature_logits[feature_index][row_index, token_index])\n",
    "                                for feature_index in range(1, len(feature_logits))]  # Omits the first feature\n",
    "                \n",
    "                # (prob, id)\n",
    "                most_probable_label = (None, None)\n",
    "                # Try each possible label, choose one with highest probability\n",
    "                for possible_label_id in possible_label_ids:\n",
    "                    possible_label_features = feature_map[possible_label_id]\n",
    "                    label_prob = 0\n",
    "                    # Sum up log probabilities for each true feature of the label\n",
    "                    for feature_index, feature_value in enumerate(possible_label_features):\n",
    "                        if feature_index == 0:\n",
    "                            continue\n",
    "                        label_prob += feature_probs[feature_index - 1][feature_value]\n",
    "\n",
    "                    if most_probable_label[0] is None or label_prob > most_probable_label[0]:\n",
    "                        most_probable_label = (label_prob, possible_label_id)\n",
    "                yield most_probable_label[1]\n",
    "\n",
    "        next_tokens = [list(_decode_row(row_index)) for row_index in range(batch_size)]\n",
    "        return torch.tensor(next_tokens)\n",
    "\n",
    "primary_feature = torch.tensor([[[0.1, 0.2, 0.1], [0.1, 0.2, 0.1]],\n",
    "                                [[0.05, 0.01, 10], [0.05, 0.01, 10]]])\n",
    "secondary_feature =  torch.tensor([[[8, 2, 1], [8, 2, 1]],\n",
    "                                [[1, 5, 10], [1, 5, 10]]], dtype=float)\n",
    "feature_map = [[0, 0], [0, 1], [1, 0], [1,1], [2,0], [2,2]]\n",
    "greedy_decode(feature_map, [primary_feature, secondary_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2257e-04, 6.6920e-03, 9.9319e-01], dtype=torch.float64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.softmax(torch.tensor([1, 5, 10], dtype=float), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.0068e+00, -5.0068e+00, -6.8379e-03], dtype=torch.float64)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.nn.LogSoftmax(dim=0)\n",
    "softmax(torch.tensor([1, 5, 10], dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in dataset['train']['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['transcription', 'segmentation', 'pos_glosses', 'glosses', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9774\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['transcription', 'segmentation', 'pos_glosses', 'glosses', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 232\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['transcription', 'segmentation', 'pos_glosses', 'glosses', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 633\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 4, 3, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('../features_v1.csv')\n",
    "(df.max(axis=0, numeric_only=True) + 1).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Gloss'] == 'TOP'].values[0][1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
