{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 unique glosses\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f3e2c44c7042c6972e5e0b152f9ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56f9eae5511430099e0d3c73ab2f189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ba9249bc3c43a3818036ffd6dcd8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/633 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'transcription': 'o sey xtok rixoqiil',\n",
       " 'segmentation': \"o' sea x-tok r-ixÃ³qiil\",\n",
       " 'pos_glosses': 'CONJ ADV COM-VT E3S-S',\n",
       " 'glosses': 'o sea COM-buscar E3S-esposa',\n",
       " 'translation': 'O sea busca esposa.',\n",
       " 'input_ids': [259, 268, 303, 276, 259, 329, 11207, 1418, 329, 159121, 696, 1],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [19, 0, 8, 0, 17, 66, 0, 31, 56, 1]}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import MT5Tokenizer\n",
    "from functools import reduce\n",
    "\n",
    "dataset = datasets.load_dataset(\"lecslab/usp-igt\")\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\", legacy=False)\n",
    "\n",
    "# Collect the unique set of gloss labels\n",
    "all_glosses = sorted(set([gloss for glosses in dataset['train']['pos_glosses'] +\n",
    "                            dataset['eval']['pos_glosses'] +\n",
    "                            dataset['test']['pos_glosses'] for gloss in glosses.replace(\"-\", \" \").split()]))\n",
    "all_glosses = [\"<sep>\", \"<pad>\"] + all_glosses\n",
    "SEP_TOKEN_ID = all_glosses.index(\"<sep>\")\n",
    "PAD_TOKEN_ID = all_glosses.index(\"<pad>\")\n",
    "print(f\"{len(all_glosses)} unique glosses\")\n",
    "\n",
    "def encode_gloss_labels(label_string: str):\n",
    "    \"\"\"Encodes glosses as an id sequence. Each morpheme gloss is assigned a unique id.\"\"\"\n",
    "    word_glosses = label_string.split()\n",
    "    glosses = [word_gloss.split(\"-\") for word_gloss in word_glosses]\n",
    "    glosses = [[all_glosses.index(gloss) for gloss in word if gloss != ''] for word in glosses]\n",
    "    glosses = reduce(lambda a, b: a + [SEP_TOKEN_ID] + b, glosses)\n",
    "    return glosses + [PAD_TOKEN_ID]\n",
    "\n",
    "def tokenize(batch):\n",
    "    inputs = tokenizer(batch['transcription'], truncation=True, padding=False, max_length=MAX_INPUT_LENGTH)\n",
    "    inputs['labels'] = [encode_gloss_labels(label) for label in batch['pos_glosses']]\n",
    "    return inputs\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  19,    0,    8,    0,   17,   66,    0,   31,   56,    1, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  43,    0,    8,    0,   51,    0,   33,    0,   21,    0,   31,\n",
      "            56,    1, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   8,    0,   21,    0,   49,   12,    0,   56,    0,   46,    1,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   7,   48,    0,   56,    0,   46,    1, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  59,    0,    8,    1, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  53,    0,    7,   32,    0,   31,   56,   48,    0,    8,    1,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   7,   32,    0,    8,    0,   56,    0,   53,    1, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  33,    0,   31,   56,   48,    1, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  56,   48,    1, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   8,    0,   56,    0,   53,    1, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  19,    0,    9,    0,   31,   56,   48,    1, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   8,    0,   38,    0,   36,   31,   66,    0,   46,    0,   31,\n",
      "             7,   57,    0,   45,    1, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   8,    0,   53,    0,   17,   66,    0,   31,   56,    1, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  46,    0,   31,   56,    0,   36,   64,    0,   48,    1, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  19,    0,   46,    0,    8,    1, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  46,    0,   56,    0,   31,   59,   48,    1, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  19,    0,   17,   66,    0,   48,    0,   14,    0,   56,    0,\n",
      "            64,    0,   31,   66,   58,    0,   48,    0,    9,    0,   56,\n",
      "             1]],\n",
      "\n",
      "        [[  64,    0,   24,   66,   58,    0,   14,    0,   56,    0,   20,\n",
      "             1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  38,    8,    0,   36,   64,    0,   20,    0,   31,   56,    1,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   8,    0,   43,    0,    8,    0,   56,    0,   53,    1, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  36,   24,   66,    0,   53,    1, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  64,    1, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  64,    0,   14,    0,   28,   56,    1, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   8,    1, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  64,    1, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  64,    0,   66,   58,    0,   48,    0,    7,    0,   56,    1,\n",
      "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  19,    0,    9,    0,   56,    0,   36,   64,    0,   56,    0,\n",
      "             9,   56,    1, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[   8,    0,   46,    0,   31,   56,   48,    0,   36,   31,   66,\n",
      "            47,    0,   31,   56,    0,    9,    0,   31,   56,    1, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  36,   31,   66,    0,   51,   30,   59,   48,    0,   20,    0,\n",
      "            20,    1, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  20,    0,   20,    0,   36,   31,   64,   47,    0,   31,   56,\n",
      "            48,    0,   51,   59,    0,   56,    1, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  19,    0,   17,    7,   39,    0,   51,    0,   45,    0,   56,\n",
      "             1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]],\n",
      "\n",
      "        [[  17,   66,    0,    9,    0,   56,    0,   46,    0,    9,    0,\n",
      "            31,   56,    1, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "          -100]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "# collator(batch.select_columns(['input_ids', 'attention_mask', 'labels']).to_list()\n",
    "\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset['train'].select_columns(['input_ids', 'attention_mask', 'labels']), batch_size=32, collate_fn=collator)\n",
    "\n",
    "for batch in dataloader:\n",
    "    # print(batch['input_ids'].shape)\n",
    "    print(batch['labels'].unsqueeze(-2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18,  0,  7,  0, 16, 65,  0, 30, 55]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(dataset['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2],\n",
       "        [5, 5]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def greedy_decode(feature_map, feature_logits: List[torch.Tensor]):\n",
    "        \"\"\"Decodes a bundle of feature logits into gloss ID predictions. \n",
    "        The output should align with the input vocabulary of the decoder.\n",
    "\n",
    "        Args:\n",
    "            feature_logits (List[torch.Tensor]): List of feature logit tensors each of size `(batch_size, seq_length, feature_size)`\n",
    "        \"\"\"\n",
    "        batch_size = feature_logits[0].shape[0]\n",
    "        seq_length = feature_logits[0].shape[1]\n",
    "\n",
    "        primary_features = torch.argmax(feature_logits[0], -1)\n",
    "\n",
    "        def _decode_row(row_index: int):\n",
    "            \"\"\"Decodes the tokens in a given row of the batch. Generator function that `yields` a single prediction (id) for each token.\"\"\"\n",
    "            for token_index in range(seq_length):\n",
    "                # Filter based on the primary feature\n",
    "                possible_label_ids = [index for index, value in enumerate(\n",
    "                    feature_map) if value[0] == primary_features[row_index, token_index].item()]\n",
    "\n",
    "                if len(possible_label_ids) == 1:\n",
    "                    yield possible_label_ids[0]\n",
    "                    continue\n",
    "\n",
    "                # Now we have a list of possible feature matrices\n",
    "                # Compute the joint probability based on the logits of each feature\n",
    "                softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "                # (num_features, feature_size)\n",
    "                feature_probs = [softmax(feature_logits[feature_index][row_index, token_index])\n",
    "                                for feature_index in range(1, len(feature_logits))]  # Omits the first feature\n",
    "                \n",
    "                # (prob, id)\n",
    "                most_probable_label = (None, None)\n",
    "                # Try each possible label, choose one with highest probability\n",
    "                for possible_label_id in possible_label_ids:\n",
    "                    possible_label_features = feature_map[possible_label_id]\n",
    "                    label_prob = 0\n",
    "                    # Sum up log probabilities for each true feature of the label\n",
    "                    for feature_index, feature_value in enumerate(possible_label_features):\n",
    "                        if feature_index == 0:\n",
    "                            continue\n",
    "                        label_prob += feature_probs[feature_index - 1][feature_value]\n",
    "\n",
    "                    if most_probable_label[0] is None or label_prob > most_probable_label[0]:\n",
    "                        most_probable_label = (label_prob, possible_label_id)\n",
    "                yield most_probable_label[1]\n",
    "\n",
    "        next_tokens = [list(_decode_row(row_index)) for row_index in range(batch_size)]\n",
    "        return torch.tensor(next_tokens)\n",
    "\n",
    "primary_feature = torch.tensor([[[0.1, 0.2, 0.1], [0.1, 0.2, 0.1]],\n",
    "                                [[0.05, 0.01, 10], [0.05, 0.01, 10]]])\n",
    "secondary_feature =  torch.tensor([[[8, 2, 1], [8, 2, 1]],\n",
    "                                [[1, 5, 10], [1, 5, 10]]], dtype=float)\n",
    "feature_map = [[0, 0], [0, 1], [1, 0], [1,1], [2,0], [2,2]]\n",
    "greedy_decode(feature_map, [primary_feature, secondary_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2257e-04, 6.6920e-03, 9.9319e-01], dtype=torch.float64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.softmax(torch.tensor([1, 5, 10], dtype=float), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.0068e+00, -5.0068e+00, -6.8379e-03], dtype=torch.float64)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.nn.LogSoftmax(dim=0)\n",
    "softmax(torch.tensor([1, 5, 10], dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in dataset['train']['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['transcription', 'segmentation', 'pos_glosses', 'glosses', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9774\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['transcription', 'segmentation', 'pos_glosses', 'glosses', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 232\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['transcription', 'segmentation', 'pos_glosses', 'glosses', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 633\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
